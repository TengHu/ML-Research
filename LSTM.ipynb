{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108461510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import torch\n",
    "import pylab\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "### File IO\n",
    "def find_files(path): return glob.glob(path)\n",
    "\n",
    "def read_lines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read() # remove newline\n",
    "    return lines\n",
    "\n",
    "def input2Tensor(line):\n",
    "    tensor = torch.zeros(1, len(line), n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[0][li][char_to_index[letter]] = 1 ## (batch, seq, n_letters)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "TEXT_PATH = 'wiki/enwik8_tail_1000'\n",
    "\n",
    "lines = \"\"\n",
    "for filename in find_files(TEXT_PATH):\n",
    "    lines = read_lines(filename)\n",
    "\n",
    "all_letters = list(set(lines))\n",
    "data_size, n_letters = len(lines), len(all_letters)\n",
    "\n",
    "char_to_index = {ch : i for i, ch in enumerate(all_letters)}\n",
    "index_to_char = {i : ch for i, ch in enumerate(all_letters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models\n",
    "\n",
    "### input: (seq,  batch, input_size)\n",
    "### hidden: (num_layers * direction, batch, hidden_size)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.o2o = nn.Linear(output_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.dropmax = nn.Dropout(0.1)\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output_combined = torch.cat((output, hidden), 1)\n",
    "        output = self.o2o(output)\n",
    "        output = self.dropmax(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers) \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.h2o(hidden[0][-1]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (Variable(torch.randn(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                Variable(torch.randn(self.num_layers, self.batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 20\n",
    "\n",
    "\n",
    "epoch = 3\n",
    "print_every = 10\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "for idx in range(0, data_size - sequence):\n",
    "    #print (lines[idx : idx + sequence], \" : \", lines[idx + sequence])\n",
    "    inputs.append(input2Tensor(lines[idx : idx + sequence]))\n",
    "    targets.append(char_to_index[lines[idx + sequence]])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat(inputs, 0)\n",
    "targets = torch.LongTensor(targets)\n",
    "dataset = torch.utils.data.TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch : 0, Iteration 0, Loss per 10 :  0.2703571915626526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Iteration 10, Loss per 10 :  0.22966250777244568\n",
      "Epoch : 0, Iteration 20, Loss per 10 :  0.17656561732292175\n",
      "Epoch : 0, Iteration 30, Loss per 10 :  0.1785702258348465\n",
      "Epoch : 0, Iteration 40, Loss per 10 :  0.1730126142501831\n",
      "Epoch : 0, Iteration 50, Loss per 10 :  0.17196720838546753\n",
      "Epoch : 0, Iteration 60, Loss per 10 :  0.1706988662481308\n",
      "Epoch : 0, Iteration 70, Loss per 10 :  0.17898812890052795\n",
      "Epoch : 0, Iteration 80, Loss per 10 :  0.17518314719200134\n",
      "Epoch : 0, Iteration 90, Loss per 10 :  0.17538107931613922\n",
      "Epoch : 0, Iteration 100, Loss per 10 :  0.16313430666923523\n",
      "Epoch : 0, Iteration 110, Loss per 10 :  0.17111876606941223\n",
      "Epoch : 0, Iteration 120, Loss per 10 :  0.18074336647987366\n",
      "Epoch : 0, Iteration 130, Loss per 10 :  0.17808860540390015\n",
      "Epoch : 0, Iteration 140, Loss per 10 :  0.16690663993358612\n",
      "Epoch : 0, Iteration 150, Loss per 10 :  0.17105980217456818\n",
      "Epoch : 0, Iteration 160, Loss per 10 :  0.16529373824596405\n",
      "Epoch : 0, Iteration 170, Loss per 10 :  0.18002335727214813\n",
      "Epoch : 0, Iteration 180, Loss per 10 :  0.17135168612003326\n",
      "Epoch : 0, Iteration 190, Loss per 10 :  0.194520965218544\n",
      "Epoch : 0, Iteration 200, Loss per 10 :  0.17760887742042542\n",
      "Epoch : 0, Iteration 210, Loss per 10 :  0.17667949199676514\n",
      "Epoch : 0, Iteration 220, Loss per 10 :  0.1625451296567917\n",
      "Epoch : 0, Iteration 230, Loss per 10 :  0.17484234273433685\n",
      "Epoch : 0, Iteration 240, Loss per 10 :  0.17097054421901703\n",
      "Epoch : 0, Iteration 250, Loss per 10 :  0.18427851796150208\n",
      "Epoch : 0, Iteration 260, Loss per 10 :  0.17636947333812714\n",
      "Epoch : 0, Iteration 270, Loss per 10 :  0.1768576055765152\n",
      "Epoch : 0, Iteration 280, Loss per 10 :  0.17640098929405212\n",
      "Epoch : 0, Iteration 290, Loss per 10 :  0.17344176769256592\n",
      "Epoch : 0, Iteration 300, Loss per 10 :  0.18212807178497314\n",
      "Epoch : 0, Iteration 310, Loss per 10 :  0.17053234577178955\n",
      "Epoch : 0, Iteration 320, Loss per 10 :  0.1801643669605255\n",
      "Epoch : 0, Iteration 330, Loss per 10 :  0.18123532831668854\n",
      "Epoch : 0, Iteration 340, Loss per 10 :  0.17874860763549805\n",
      "Epoch : 0, Iteration 350, Loss per 10 :  0.15645429491996765\n",
      "Epoch : 0, Iteration 360, Loss per 10 :  0.16965684294700623\n",
      "Epoch : 0, Iteration 370, Loss per 10 :  0.16970284283161163\n",
      "Epoch : 0, Iteration 380, Loss per 10 :  0.17454731464385986\n",
      "Epoch : 0, Iteration 390, Loss per 10 :  0.176126629114151\n",
      "Epoch : 0, Iteration 400, Loss per 10 :  0.17077746987342834\n",
      "Epoch : 0, Iteration 410, Loss per 10 :  0.1694585382938385\n",
      "Epoch : 0, Iteration 420, Loss per 10 :  0.17188307642936707\n",
      "Epoch : 0, Iteration 430, Loss per 10 :  0.17631621658802032\n",
      "Epoch : 0, Iteration 440, Loss per 10 :  0.17565712332725525\n",
      "Epoch : 0, Iteration 450, Loss per 10 :  0.18239855766296387\n",
      "Epoch : 0, Iteration 460, Loss per 10 :  0.18008911609649658\n",
      "Epoch : 0, Iteration 470, Loss per 10 :  0.17841297388076782\n",
      "Epoch : 0, Iteration 480, Loss per 10 :  0.1799917072057724\n",
      "Epoch : 0, Iteration 490, Loss per 10 :  0.1769036054611206\n",
      "Epoch : 0, Iteration 500, Loss per 10 :  0.17823557555675507\n",
      "Epoch : 0, Iteration 510, Loss per 10 :  0.1695024073123932\n",
      "Epoch : 0, Iteration 520, Loss per 10 :  0.17747986316680908\n",
      "Epoch : 0, Iteration 530, Loss per 10 :  0.1591881811618805\n",
      "Epoch : 0, Iteration 540, Loss per 10 :  0.16559836268424988\n",
      "Epoch : 0, Iteration 550, Loss per 10 :  0.16027121245861053\n",
      "Epoch : 0, Iteration 560, Loss per 10 :  0.18550187349319458\n",
      "Epoch : 0, Iteration 570, Loss per 10 :  0.16849029064178467\n",
      "Epoch : 0, Iteration 580, Loss per 10 :  0.1738615334033966\n",
      "Epoch : 0, Iteration 590, Loss per 10 :  0.17055729031562805\n",
      "Epoch : 0, Iteration 600, Loss per 10 :  0.1716751754283905\n",
      "Epoch : 0, Iteration 610, Loss per 10 :  0.1775510311126709\n",
      "Epoch : 0, Iteration 620, Loss per 10 :  0.17085973918437958\n",
      "Epoch : 0, Iteration 630, Loss per 10 :  0.1782568395137787\n",
      "Epoch : 0, Iteration 640, Loss per 10 :  0.17227765917778015\n",
      "Epoch : 0, Iteration 650, Loss per 10 :  0.17381848394870758\n",
      "Epoch : 0, Iteration 660, Loss per 10 :  0.17992377281188965\n",
      "Epoch : 0, Iteration 670, Loss per 10 :  0.17502561211585999\n",
      "Epoch : 0, Iteration 680, Loss per 10 :  0.17778868973255157\n",
      "Epoch : 0, Iteration 690, Loss per 10 :  0.18097427487373352\n",
      "Epoch : 0, Iteration 700, Loss per 10 :  0.17037245631217957\n",
      "Epoch : 0, Iteration 710, Loss per 10 :  0.17504985630512238\n",
      "Epoch : 0, Iteration 720, Loss per 10 :  0.16163548827171326\n",
      "Epoch : 0, Iteration 730, Loss per 10 :  0.17623752355575562\n",
      "Epoch : 0, Iteration 740, Loss per 10 :  0.17140577733516693\n",
      "Epoch : 0, Iteration 750, Loss per 10 :  0.1687023788690567\n",
      "Epoch : 0, Iteration 760, Loss per 10 :  0.17545507848262787\n",
      "Epoch : 0, Iteration 770, Loss per 10 :  0.1637800931930542\n",
      "Epoch : 0, Iteration 780, Loss per 10 :  0.17212796211242676\n",
      "Epoch : 0, Iteration 790, Loss per 10 :  0.17112776637077332\n",
      "Epoch : 0, Iteration 800, Loss per 10 :  0.17370544373989105\n",
      "Epoch : 0, Iteration 810, Loss per 10 :  0.1771320402622223\n",
      "Epoch : 0, Iteration 820, Loss per 10 :  0.1827080249786377\n",
      "Epoch : 0, Iteration 830, Loss per 10 :  0.17222371697425842\n",
      "Epoch : 0, Iteration 840, Loss per 10 :  0.17287132143974304\n",
      "Epoch : 0, Iteration 850, Loss per 10 :  0.17541120946407318\n",
      "Epoch : 0, Iteration 860, Loss per 10 :  0.1808711439371109\n",
      "Epoch : 0, Iteration 870, Loss per 10 :  0.16807493567466736\n",
      "Epoch : 0, Iteration 880, Loss per 10 :  0.17871218919754028\n",
      "Epoch : 0, Iteration 890, Loss per 10 :  0.1628427803516388\n",
      "Epoch : 0, Iteration 900, Loss per 10 :  0.17464305460453033\n",
      "Epoch : 0, Iteration 910, Loss per 10 :  0.17918500304222107\n",
      "Epoch : 0, Iteration 920, Loss per 10 :  0.17304983735084534\n",
      "Epoch : 0, Iteration 930, Loss per 10 :  0.167882040143013\n",
      "Epoch : 0, Iteration 940, Loss per 10 :  0.1709458827972412\n",
      "Epoch : 0, Iteration 950, Loss per 10 :  0.17575040459632874\n",
      "Epoch : 0, Iteration 960, Loss per 10 :  0.17607787251472473\n",
      "Epoch : 0, Iteration 970, Loss per 10 :  0.18177065253257751\n",
      "Epoch : 0, Iteration 980, Loss per 10 :  0.1723451465368271\n",
      "Epoch : 0, Iteration 990, Loss per 10 :  0.18493951857089996\n",
      "Epoch : 0, Iteration 1000, Loss per 10 :  0.17829415202140808\n",
      "Epoch : 0, Iteration 1010, Loss per 10 :  0.17112959921360016\n",
      "Epoch : 0, Iteration 1020, Loss per 10 :  0.17265114188194275\n",
      "Epoch : 0, Iteration 1030, Loss per 10 :  0.17592382431030273\n",
      "Epoch : 0, Iteration 1040, Loss per 10 :  0.1796923577785492\n",
      "Epoch : 0, Iteration 1050, Loss per 10 :  0.17792324721813202\n",
      "Epoch : 0, Iteration 1060, Loss per 10 :  0.1658254712820053\n",
      "Epoch : 0, Iteration 1070, Loss per 10 :  0.16338609158992767\n",
      "Epoch : 0, Iteration 1080, Loss per 10 :  0.17448633909225464\n",
      "Epoch : 0, Iteration 1090, Loss per 10 :  0.1683966964483261\n",
      "Epoch : 0, Iteration 1100, Loss per 10 :  0.17231732606887817\n",
      "Epoch : 0, Iteration 1110, Loss per 10 :  0.17004109919071198\n",
      "Epoch : 0, Iteration 1120, Loss per 10 :  0.16641651093959808\n",
      "Epoch : 0, Iteration 1130, Loss per 10 :  0.17353880405426025\n",
      "Epoch : 0, Iteration 1140, Loss per 10 :  0.16427071392536163\n",
      "Epoch : 0, Iteration 1150, Loss per 10 :  0.16969147324562073\n",
      "Epoch : 0, Iteration 1160, Loss per 10 :  0.16881194710731506\n",
      "Epoch : 0, Iteration 1170, Loss per 10 :  0.17971543967723846\n",
      "Epoch : 0, Iteration 1180, Loss per 10 :  0.17326849699020386\n",
      "Epoch : 0, Iteration 1190, Loss per 10 :  0.17253610491752625\n",
      "Epoch : 0, Iteration 1200, Loss per 10 :  0.16837897896766663\n",
      "Epoch : 0, Iteration 1210, Loss per 10 :  0.1670869141817093\n",
      "not enough batch, break\n",
      "Epoch : 1, Iteration 0, Loss per 10 :  0.1745242178440094\n",
      "Epoch : 1, Iteration 10, Loss per 10 :  0.17587164044380188\n",
      "Epoch : 1, Iteration 20, Loss per 10 :  0.1690899282693863\n",
      "Epoch : 1, Iteration 30, Loss per 10 :  0.18576034903526306\n",
      "Epoch : 1, Iteration 40, Loss per 10 :  0.1737658679485321\n",
      "Epoch : 1, Iteration 50, Loss per 10 :  0.1867985725402832\n",
      "Epoch : 1, Iteration 60, Loss per 10 :  0.17195819318294525\n",
      "Epoch : 1, Iteration 70, Loss per 10 :  0.17080743610858917\n",
      "Epoch : 1, Iteration 80, Loss per 10 :  0.16118261218070984\n",
      "Epoch : 1, Iteration 90, Loss per 10 :  0.17369668185710907\n",
      "Epoch : 1, Iteration 100, Loss per 10 :  0.17402991652488708\n",
      "Epoch : 1, Iteration 110, Loss per 10 :  0.17142775654792786\n",
      "Epoch : 1, Iteration 120, Loss per 10 :  0.18091070652008057\n",
      "Epoch : 1, Iteration 130, Loss per 10 :  0.1741827428340912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Iteration 140, Loss per 10 :  0.1775980144739151\n",
      "Epoch : 1, Iteration 150, Loss per 10 :  0.17563840746879578\n",
      "Epoch : 1, Iteration 160, Loss per 10 :  0.15753671526908875\n",
      "Epoch : 1, Iteration 170, Loss per 10 :  0.1714009791612625\n",
      "Epoch : 1, Iteration 180, Loss per 10 :  0.175504669547081\n",
      "Epoch : 1, Iteration 190, Loss per 10 :  0.17257457971572876\n",
      "Epoch : 1, Iteration 200, Loss per 10 :  0.18143771588802338\n",
      "Epoch : 1, Iteration 210, Loss per 10 :  0.16439132392406464\n",
      "Epoch : 1, Iteration 220, Loss per 10 :  0.17676718533039093\n",
      "Epoch : 1, Iteration 230, Loss per 10 :  0.16900727152824402\n",
      "Epoch : 1, Iteration 240, Loss per 10 :  0.17109271883964539\n",
      "Epoch : 1, Iteration 250, Loss per 10 :  0.17707495391368866\n",
      "Epoch : 1, Iteration 260, Loss per 10 :  0.16843447089195251\n",
      "Epoch : 1, Iteration 270, Loss per 10 :  0.17231570184230804\n",
      "Epoch : 1, Iteration 280, Loss per 10 :  0.1831255853176117\n",
      "Epoch : 1, Iteration 290, Loss per 10 :  0.1729181706905365\n",
      "Epoch : 1, Iteration 300, Loss per 10 :  0.16100876033306122\n",
      "Epoch : 1, Iteration 310, Loss per 10 :  0.17587584257125854\n",
      "Epoch : 1, Iteration 320, Loss per 10 :  0.1744009256362915\n",
      "Epoch : 1, Iteration 330, Loss per 10 :  0.17481616139411926\n",
      "Epoch : 1, Iteration 340, Loss per 10 :  0.1724637895822525\n",
      "Epoch : 1, Iteration 350, Loss per 10 :  0.15923556685447693\n",
      "Epoch : 1, Iteration 360, Loss per 10 :  0.1758406013250351\n",
      "Epoch : 1, Iteration 370, Loss per 10 :  0.17503449320793152\n",
      "Epoch : 1, Iteration 380, Loss per 10 :  0.18029436469078064\n",
      "Epoch : 1, Iteration 390, Loss per 10 :  0.17677660286426544\n",
      "Epoch : 1, Iteration 400, Loss per 10 :  0.16608163714408875\n",
      "Epoch : 1, Iteration 410, Loss per 10 :  0.1705566793680191\n",
      "Epoch : 1, Iteration 420, Loss per 10 :  0.17626214027404785\n",
      "Epoch : 1, Iteration 430, Loss per 10 :  0.1684681624174118\n",
      "Epoch : 1, Iteration 440, Loss per 10 :  0.17775455117225647\n",
      "Epoch : 1, Iteration 450, Loss per 10 :  0.18946729600429535\n",
      "Epoch : 1, Iteration 460, Loss per 10 :  0.16776619851589203\n",
      "Epoch : 1, Iteration 470, Loss per 10 :  0.17818962037563324\n",
      "Epoch : 1, Iteration 480, Loss per 10 :  0.1787978559732437\n",
      "Epoch : 1, Iteration 490, Loss per 10 :  0.17843939363956451\n",
      "Epoch : 1, Iteration 500, Loss per 10 :  0.1747138798236847\n",
      "Epoch : 1, Iteration 510, Loss per 10 :  0.1625790148973465\n",
      "Epoch : 1, Iteration 520, Loss per 10 :  0.17431005835533142\n",
      "Epoch : 1, Iteration 530, Loss per 10 :  0.1784755289554596\n",
      "Epoch : 1, Iteration 540, Loss per 10 :  0.17388367652893066\n",
      "Epoch : 1, Iteration 550, Loss per 10 :  0.1710820496082306\n",
      "Epoch : 1, Iteration 560, Loss per 10 :  0.19268524646759033\n",
      "Epoch : 1, Iteration 570, Loss per 10 :  0.18614454567432404\n",
      "Epoch : 1, Iteration 580, Loss per 10 :  0.18389751017093658\n",
      "Epoch : 1, Iteration 590, Loss per 10 :  0.178324893116951\n",
      "Epoch : 1, Iteration 600, Loss per 10 :  0.18580588698387146\n",
      "Epoch : 1, Iteration 610, Loss per 10 :  0.17820541560649872\n",
      "Epoch : 1, Iteration 620, Loss per 10 :  0.1693323254585266\n",
      "Epoch : 1, Iteration 630, Loss per 10 :  0.17508268356323242\n",
      "Epoch : 1, Iteration 640, Loss per 10 :  0.16749802231788635\n",
      "Epoch : 1, Iteration 650, Loss per 10 :  0.1648472547531128\n",
      "Epoch : 1, Iteration 660, Loss per 10 :  0.17039744555950165\n",
      "Epoch : 1, Iteration 670, Loss per 10 :  0.17046555876731873\n",
      "Epoch : 1, Iteration 680, Loss per 10 :  0.1701623499393463\n",
      "Epoch : 1, Iteration 690, Loss per 10 :  0.17017118632793427\n",
      "Epoch : 1, Iteration 700, Loss per 10 :  0.15970847010612488\n",
      "Epoch : 1, Iteration 710, Loss per 10 :  0.1700744330883026\n",
      "Epoch : 1, Iteration 720, Loss per 10 :  0.1780295968055725\n",
      "Epoch : 1, Iteration 730, Loss per 10 :  0.16330622136592865\n",
      "Epoch : 1, Iteration 740, Loss per 10 :  0.18129168450832367\n",
      "Epoch : 1, Iteration 750, Loss per 10 :  0.16726067662239075\n",
      "Epoch : 1, Iteration 760, Loss per 10 :  0.17706198990345\n",
      "Epoch : 1, Iteration 770, Loss per 10 :  0.17629460990428925\n",
      "Epoch : 1, Iteration 780, Loss per 10 :  0.16774949431419373\n",
      "Epoch : 1, Iteration 790, Loss per 10 :  0.16685181856155396\n",
      "Epoch : 1, Iteration 800, Loss per 10 :  0.16753943264484406\n",
      "Epoch : 1, Iteration 810, Loss per 10 :  0.17411503195762634\n",
      "Epoch : 1, Iteration 820, Loss per 10 :  0.174289733171463\n",
      "Epoch : 1, Iteration 830, Loss per 10 :  0.1773609071969986\n",
      "Epoch : 1, Iteration 840, Loss per 10 :  0.1733691543340683\n",
      "Epoch : 1, Iteration 850, Loss per 10 :  0.17406830191612244\n",
      "Epoch : 1, Iteration 860, Loss per 10 :  0.17170454561710358\n",
      "Epoch : 1, Iteration 870, Loss per 10 :  0.17170725762844086\n",
      "Epoch : 1, Iteration 880, Loss per 10 :  0.18483129143714905\n",
      "Epoch : 1, Iteration 890, Loss per 10 :  0.16289548575878143\n",
      "Epoch : 1, Iteration 900, Loss per 10 :  0.17211678624153137\n",
      "Epoch : 1, Iteration 910, Loss per 10 :  0.16871069371700287\n",
      "Epoch : 1, Iteration 920, Loss per 10 :  0.1716325581073761\n",
      "Epoch : 1, Iteration 930, Loss per 10 :  0.1698499470949173\n",
      "Epoch : 1, Iteration 940, Loss per 10 :  0.17182230949401855\n",
      "Epoch : 1, Iteration 950, Loss per 10 :  0.1651746928691864\n",
      "Epoch : 1, Iteration 960, Loss per 10 :  0.1817394196987152\n",
      "Epoch : 1, Iteration 970, Loss per 10 :  0.1685541272163391\n",
      "Epoch : 1, Iteration 980, Loss per 10 :  0.17429383099079132\n",
      "Epoch : 1, Iteration 990, Loss per 10 :  0.16471019387245178\n",
      "Epoch : 1, Iteration 1000, Loss per 10 :  0.17253980040550232\n",
      "Epoch : 1, Iteration 1010, Loss per 10 :  0.17724058032035828\n",
      "Epoch : 1, Iteration 1020, Loss per 10 :  0.1702364832162857\n",
      "Epoch : 1, Iteration 1030, Loss per 10 :  0.1758190542459488\n",
      "Epoch : 1, Iteration 1040, Loss per 10 :  0.17396996915340424\n",
      "Epoch : 1, Iteration 1050, Loss per 10 :  0.18066295981407166\n",
      "Epoch : 1, Iteration 1060, Loss per 10 :  0.16594597697257996\n",
      "Epoch : 1, Iteration 1070, Loss per 10 :  0.16105961799621582\n",
      "Epoch : 1, Iteration 1080, Loss per 10 :  0.17610417306423187\n",
      "Epoch : 1, Iteration 1090, Loss per 10 :  0.17536026239395142\n",
      "Epoch : 1, Iteration 1100, Loss per 10 :  0.17588898539543152\n",
      "Epoch : 1, Iteration 1110, Loss per 10 :  0.1687564253807068\n",
      "Epoch : 1, Iteration 1120, Loss per 10 :  0.17677585780620575\n",
      "Epoch : 1, Iteration 1130, Loss per 10 :  0.1729796826839447\n",
      "Epoch : 1, Iteration 1140, Loss per 10 :  0.1802913248538971\n",
      "Epoch : 1, Iteration 1150, Loss per 10 :  0.16657555103302002\n",
      "Epoch : 1, Iteration 1160, Loss per 10 :  0.16421855986118317\n",
      "Epoch : 1, Iteration 1170, Loss per 10 :  0.1791846603155136\n",
      "Epoch : 1, Iteration 1180, Loss per 10 :  0.17290426790714264\n",
      "Epoch : 1, Iteration 1190, Loss per 10 :  0.17226797342300415\n",
      "Epoch : 1, Iteration 1200, Loss per 10 :  0.16629542410373688\n",
      "Epoch : 1, Iteration 1210, Loss per 10 :  0.16995184123516083\n",
      "not enough batch, break\n",
      "Epoch : 2, Iteration 0, Loss per 10 :  0.15617796778678894\n",
      "Epoch : 2, Iteration 10, Loss per 10 :  0.18015563488006592\n",
      "Epoch : 2, Iteration 20, Loss per 10 :  0.17525318264961243\n",
      "Epoch : 2, Iteration 30, Loss per 10 :  0.17087440192699432\n",
      "Epoch : 2, Iteration 40, Loss per 10 :  0.17780755460262299\n",
      "Epoch : 2, Iteration 50, Loss per 10 :  0.1781732738018036\n",
      "Epoch : 2, Iteration 60, Loss per 10 :  0.17409516870975494\n",
      "Epoch : 2, Iteration 70, Loss per 10 :  0.1757228523492813\n",
      "Epoch : 2, Iteration 80, Loss per 10 :  0.17019504308700562\n",
      "Epoch : 2, Iteration 90, Loss per 10 :  0.17912806570529938\n",
      "Epoch : 2, Iteration 100, Loss per 10 :  0.17785239219665527\n",
      "Epoch : 2, Iteration 110, Loss per 10 :  0.17190125584602356\n",
      "Epoch : 2, Iteration 120, Loss per 10 :  0.18207091093063354\n",
      "Epoch : 2, Iteration 130, Loss per 10 :  0.17464181780815125\n",
      "Epoch : 2, Iteration 140, Loss per 10 :  0.18247675895690918\n",
      "Epoch : 2, Iteration 150, Loss per 10 :  0.18051081895828247\n",
      "Epoch : 2, Iteration 160, Loss per 10 :  0.17338353395462036\n",
      "Epoch : 2, Iteration 170, Loss per 10 :  0.17430342733860016\n",
      "Epoch : 2, Iteration 180, Loss per 10 :  0.17954036593437195\n",
      "Epoch : 2, Iteration 190, Loss per 10 :  0.1744600087404251\n",
      "Epoch : 2, Iteration 200, Loss per 10 :  0.172775536775589\n",
      "Epoch : 2, Iteration 210, Loss per 10 :  0.18187470734119415\n",
      "Epoch : 2, Iteration 220, Loss per 10 :  0.1681182086467743\n",
      "Epoch : 2, Iteration 230, Loss per 10 :  0.18224401772022247\n",
      "Epoch : 2, Iteration 240, Loss per 10 :  0.1710810363292694\n",
      "Epoch : 2, Iteration 250, Loss per 10 :  0.1746055781841278\n",
      "Epoch : 2, Iteration 260, Loss per 10 :  0.17818588018417358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, Iteration 270, Loss per 10 :  0.18018914759159088\n",
      "Epoch : 2, Iteration 280, Loss per 10 :  0.1705566942691803\n",
      "Epoch : 2, Iteration 290, Loss per 10 :  0.17589163780212402\n",
      "Epoch : 2, Iteration 300, Loss per 10 :  0.17702490091323853\n",
      "Epoch : 2, Iteration 310, Loss per 10 :  0.17562493681907654\n",
      "Epoch : 2, Iteration 320, Loss per 10 :  0.17313405871391296\n",
      "Epoch : 2, Iteration 330, Loss per 10 :  0.1731942594051361\n",
      "Epoch : 2, Iteration 340, Loss per 10 :  0.1615360677242279\n",
      "Epoch : 2, Iteration 350, Loss per 10 :  0.1628122627735138\n",
      "Epoch : 2, Iteration 360, Loss per 10 :  0.1832326352596283\n",
      "Epoch : 2, Iteration 370, Loss per 10 :  0.16902202367782593\n",
      "Epoch : 2, Iteration 380, Loss per 10 :  0.16987909376621246\n",
      "Epoch : 2, Iteration 390, Loss per 10 :  0.168397918343544\n",
      "Epoch : 2, Iteration 400, Loss per 10 :  0.17869079113006592\n",
      "Epoch : 2, Iteration 410, Loss per 10 :  0.16926078498363495\n",
      "Epoch : 2, Iteration 420, Loss per 10 :  0.1725364774465561\n",
      "Epoch : 2, Iteration 430, Loss per 10 :  0.17103548347949982\n",
      "Epoch : 2, Iteration 440, Loss per 10 :  0.17350010573863983\n",
      "Epoch : 2, Iteration 450, Loss per 10 :  0.17103253304958344\n",
      "Epoch : 2, Iteration 460, Loss per 10 :  0.17355336248874664\n",
      "Epoch : 2, Iteration 470, Loss per 10 :  0.1873416155576706\n",
      "Epoch : 2, Iteration 480, Loss per 10 :  0.1710297167301178\n",
      "Epoch : 2, Iteration 490, Loss per 10 :  0.16699454188346863\n",
      "Epoch : 2, Iteration 500, Loss per 10 :  0.16315051913261414\n",
      "Epoch : 2, Iteration 510, Loss per 10 :  0.17439277470111847\n",
      "Epoch : 2, Iteration 520, Loss per 10 :  0.16716347634792328\n",
      "Epoch : 2, Iteration 530, Loss per 10 :  0.1712374985218048\n",
      "Epoch : 2, Iteration 540, Loss per 10 :  0.1773225963115692\n",
      "Epoch : 2, Iteration 550, Loss per 10 :  0.17317986488342285\n",
      "Epoch : 2, Iteration 560, Loss per 10 :  0.16654440760612488\n",
      "Epoch : 2, Iteration 570, Loss per 10 :  0.17269495129585266\n",
      "Epoch : 2, Iteration 580, Loss per 10 :  0.1785668432712555\n",
      "Epoch : 2, Iteration 590, Loss per 10 :  0.1731639802455902\n",
      "Epoch : 2, Iteration 600, Loss per 10 :  0.1702524870634079\n",
      "Epoch : 2, Iteration 610, Loss per 10 :  0.17614403367042542\n",
      "Epoch : 2, Iteration 620, Loss per 10 :  0.16792313754558563\n",
      "Epoch : 2, Iteration 630, Loss per 10 :  0.16821572184562683\n",
      "Epoch : 2, Iteration 640, Loss per 10 :  0.16306358575820923\n",
      "Epoch : 2, Iteration 650, Loss per 10 :  0.17712676525115967\n",
      "Epoch : 2, Iteration 660, Loss per 10 :  0.16657193005084991\n",
      "Epoch : 2, Iteration 670, Loss per 10 :  0.16357333958148956\n",
      "Epoch : 2, Iteration 680, Loss per 10 :  0.16740283370018005\n",
      "Epoch : 2, Iteration 690, Loss per 10 :  0.1745913028717041\n",
      "Epoch : 2, Iteration 700, Loss per 10 :  0.17283697426319122\n",
      "Epoch : 2, Iteration 710, Loss per 10 :  0.17121119797229767\n",
      "Epoch : 2, Iteration 720, Loss per 10 :  0.16823746263980865\n",
      "Epoch : 2, Iteration 730, Loss per 10 :  0.18889304995536804\n",
      "Epoch : 2, Iteration 740, Loss per 10 :  0.17518289387226105\n",
      "Epoch : 2, Iteration 750, Loss per 10 :  0.17476044595241547\n",
      "Epoch : 2, Iteration 760, Loss per 10 :  0.17772755026817322\n",
      "Epoch : 2, Iteration 770, Loss per 10 :  0.17544321715831757\n",
      "Epoch : 2, Iteration 780, Loss per 10 :  0.18038541078567505\n",
      "Epoch : 2, Iteration 790, Loss per 10 :  0.17487026751041412\n",
      "Epoch : 2, Iteration 800, Loss per 10 :  0.1798936426639557\n",
      "Epoch : 2, Iteration 810, Loss per 10 :  0.17404478788375854\n",
      "Epoch : 2, Iteration 820, Loss per 10 :  0.1678813397884369\n",
      "Epoch : 2, Iteration 830, Loss per 10 :  0.17230349779129028\n",
      "Epoch : 2, Iteration 840, Loss per 10 :  0.18155845999717712\n",
      "Epoch : 2, Iteration 850, Loss per 10 :  0.17378178238868713\n",
      "Epoch : 2, Iteration 860, Loss per 10 :  0.17798413336277008\n",
      "Epoch : 2, Iteration 870, Loss per 10 :  0.1751563996076584\n",
      "Epoch : 2, Iteration 880, Loss per 10 :  0.16546620428562164\n",
      "Epoch : 2, Iteration 890, Loss per 10 :  0.1662144660949707\n",
      "Epoch : 2, Iteration 900, Loss per 10 :  0.16928379237651825\n",
      "Epoch : 2, Iteration 910, Loss per 10 :  0.16144151985645294\n",
      "Epoch : 2, Iteration 920, Loss per 10 :  0.1873215287923813\n",
      "Epoch : 2, Iteration 930, Loss per 10 :  0.1700911521911621\n",
      "Epoch : 2, Iteration 940, Loss per 10 :  0.17544592916965485\n",
      "Epoch : 2, Iteration 950, Loss per 10 :  0.17097751796245575\n",
      "Epoch : 2, Iteration 960, Loss per 10 :  0.16327045857906342\n",
      "Epoch : 2, Iteration 970, Loss per 10 :  0.17014159262180328\n",
      "Epoch : 2, Iteration 980, Loss per 10 :  0.17610126733779907\n",
      "Epoch : 2, Iteration 990, Loss per 10 :  0.172167107462883\n",
      "Epoch : 2, Iteration 1000, Loss per 10 :  0.16849058866500854\n",
      "Epoch : 2, Iteration 1010, Loss per 10 :  0.1814277321100235\n",
      "Epoch : 2, Iteration 1020, Loss per 10 :  0.17984013259410858\n",
      "Epoch : 2, Iteration 1030, Loss per 10 :  0.16665562987327576\n",
      "Epoch : 2, Iteration 1040, Loss per 10 :  0.16529618203639984\n",
      "Epoch : 2, Iteration 1050, Loss per 10 :  0.16677451133728027\n",
      "Epoch : 2, Iteration 1060, Loss per 10 :  0.16589245200157166\n",
      "Epoch : 2, Iteration 1070, Loss per 10 :  0.17620137333869934\n",
      "Epoch : 2, Iteration 1080, Loss per 10 :  0.18006257712841034\n",
      "Epoch : 2, Iteration 1090, Loss per 10 :  0.18152500689029694\n",
      "Epoch : 2, Iteration 1100, Loss per 10 :  0.16976992785930634\n",
      "Epoch : 2, Iteration 1110, Loss per 10 :  0.17784924805164337\n",
      "Epoch : 2, Iteration 1120, Loss per 10 :  0.17919093370437622\n",
      "Epoch : 2, Iteration 1130, Loss per 10 :  0.17535290122032166\n",
      "Epoch : 2, Iteration 1140, Loss per 10 :  0.16325464844703674\n",
      "Epoch : 2, Iteration 1150, Loss per 10 :  0.17591002583503723\n",
      "Epoch : 2, Iteration 1160, Loss per 10 :  0.1790064573287964\n",
      "Epoch : 2, Iteration 1170, Loss per 10 :  0.1760977804660797\n",
      "Epoch : 2, Iteration 1180, Loss per 10 :  0.16975486278533936\n",
      "Epoch : 2, Iteration 1190, Loss per 10 :  0.17192459106445312\n",
      "Epoch : 2, Iteration 1200, Loss per 10 :  0.17167691886425018\n",
      "Epoch : 2, Iteration 1210, Loss per 10 :  0.17146053910255432\n",
      "not enough batch, break\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "### Define Model\n",
    "\n",
    "num_layers = 3\n",
    "hidden_size = 128\n",
    "input_size = n_letters\n",
    "output_size = n_letters\n",
    "\n",
    "\n",
    "all_losses = []\n",
    "total_loss = 0\n",
    "\n",
    "rnn = nn.DataParallel(LSTM(n_letters, hidden_size, n_letters, batch_size, num_layers))\n",
    "\n",
    "\n",
    "### loss \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "### Training\n",
    "def train(line_tensor, target_tensor):\n",
    "    ## h_0, c_0\n",
    "    hidden = (Variable(torch.randn(num_layers, batch_size, hidden_size)),\n",
    "            Variable(torch.randn(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    output, hidden = rnn(line_tensor, hidden)\n",
    "    \n",
    "    #print (target_tensor.shape)\n",
    "    loss += criterion(output,  target_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0] / line_tensor.size()[0] \n",
    "\n",
    "print (\"Start Training\")\n",
    "\n",
    "                                      \n",
    "for epoch_iter in range(0, epoch):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for iter, data in enumerate(dataloader, 0):\n",
    "        inputTensor, targetTensor = data\n",
    "        \n",
    "        if inputTensor.shape[0] != batch_size:\n",
    "            print (\"not enough batch, break\")\n",
    "            break\n",
    "        \n",
    "        inputTensor = inputTensor.view(sequence, batch_size, n_letters)\n",
    "        \n",
    "        inputTensor, targetTensor = Variable(inputTensor, requires_grad=True), Variable(targetTensor)\n",
    "        output, loss = train(inputTensor, targetTensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_loss += loss\n",
    "        if iter % print_every == 0:\n",
    "            print (\"Epoch : {}, Iteration {}, Loss per {} :  {}\".format(epoch_iter, iter, print_every,  loss))\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'randomChoice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5871ea0283a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmax_sample_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarm_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-5871ea0283a5>\u001b[0m in \u001b[0;36msample_text\u001b[0;34m(line, sequence, temperature)\u001b[0m\n\u001b[1;32m     12\u001b[0m               Variable(torch.zeros(num_layers, 1, hidden_size)))\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomChoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'randomChoice' is not defined"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(np.exp(preds)).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def sample_text(line, sequence, temperature=1.0):\n",
    "    hidden = (Variable(torch.zeros(num_layers, 1, hidden_size)),\n",
    "              Variable(torch.zeros(num_layers, 1, hidden_size)))\n",
    "\n",
    "    string = randomChoice(line, sequence+1)\n",
    "    input = Variable(lineToTensor(string))\n",
    "    output_string = string\n",
    "\n",
    "    print (\"Starting with : \" + string)\n",
    "    \n",
    "    for i in range(max_sample_length):\n",
    "        output, hidden = rnn(input, hidden)\n",
    "        #topv, topi = output[0].data.topk(1)\n",
    "        #print (output[0].data.numpy())\n",
    "        topi = sample(output[0].data.numpy(), temperature)\n",
    "        #topi = topi[0]\n",
    "\n",
    "        if topi == n_letters - 1:\n",
    "            break\n",
    "        else:\n",
    "            letter = all_letters[topi]\n",
    "            string = string[1:] + letter\n",
    "            output_string += letter\n",
    "        input = Variable(lineToTensor(string))\n",
    "\n",
    "    print (\"Generating Text: \\n\\n\\n\")\n",
    "    return output_string\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "max_sample_length = 800\n",
    "print(sample_text(warm_file, sequence, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
